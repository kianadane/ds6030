---
title: "Homework #5: Probability and Classification" 
author: "Kiana Dane"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
dir_data= 'https://mdporter.github.io/teaching/data/' # data directory
library(glmnet)
library(tidyverse)
library(randomForest)
library(pROC)
library(ggplot2)# 
```


# Crime Linkage

Crime linkage attempts to determine if a set of unsolved crimes share a common offender. *Pairwise* crime linkage is the more simple task of deciding if two crimes share a common offender; it can be considered a binary classification problem. The linkage training data has 8 evidence variables that measure the similarity between a pair of crimes:

- `spatial` is the spatial distance between the crimes
- `temporal` is the fractional time (in days) between the crimes
- `tod` and `dow` are the differences in time of day and day of week between the crimes
- `LOC`, `POA,` and `MOA` are binary with a 1 corresponding to a match (type of property, point of entry, method of entry)
- `TIMERANGE` is the time between the earliest and latest possible times the crime could have occurred (because the victim was away from the house during the crime).
- The response variable indicates if the crimes are linked ($y=1$) or unlinked ($y=0$).


## Load Crime Linkage Data

::: {.callout-note title="Solution"}
```{r load data}
linkage_train <- read.csv("linkage_train.csv")
linkage_test <- read.csv("linkage_test.csv")

```
:::

# Problem 1: Penalized Regression for Crime Linkage

## a. Fit a penalized *linear regression* model to predict linkage. 

Use an elastic net penalty (including lasso and ridge) (your choice). 

- Report the value of $\alpha \in [0, 1]$ used. 
- Report the value of $\lambda$ used.
- Report the estimated coefficients.

::: {.callout-note title="Solution"}
```{r prob1a}
x_train <- model.matrix(~ . - y, data = linkage_train)

y_train <- linkage_train$y

cv_1 <- cv.glmnet(x_train, y_train, alpha = 0.1, family = "binomial")

m_lambda <- cv_1$lambda.min
min_lambda <- m_lambda

r_coef <- coef(cv_1, s = m_lambda)
r_coefs <- as.matrix(r_coef)

print(r_coefs)
print("Alpha used: 0.1")
cat("Min Lambda: ", format(min_lambda, digits = 5), "\n")

```

:::
## b. Fit a penalized *logistic regression* model to predict linkage. 

Use an elastic net penalty (including lasso and ridge) (your choice). 

- Report the value of $\alpha \in [0, 1]$ used. 
- Report the value of $\lambda$ used.
- Report the estimated coefficients.

::: {.callout-note title="Solution"}
```{r prob1b}
cv_2 <- cv.glmnet(x_train, y_train, alpha = 0.45, family = "binomial")
lambda_2 <- cv_2$lambda.min
lambda_2

en_coef <- coef(cv_2, s = lambda_2)
en_coefs <- as.matrix(en_coef)

print(en_coefs)
```
- alpha = 0.45
- lambda = 0.0000638
:::

# Problem 2: Random Forest for Crime Linkage

Fit a random forest model to predict crime linkage. 

- Report the loss function (or splitting rule) used. 
- Report any non-default tuning parameters.
- Report the variable importance (indicate which importance method was used). 

::: {.callout-note title="Solution"}
```{r prob2}
cv_2m <- as.matrix(cv_2)

rf_model2 <- randomForest(x_train, as.factor(y_train), ntree = 700, mtry = 4, importance = TRUE)

important_vals <- importance(rf_model2)
var_important <- as.data.frame(important_vals)

print(var_important)

```

- Loss function : Gini impurity (default)
- Tuning Parameters: 
      - 700 trees
      - 4 random variables every split
      - both importance methods tried
:::

# Problem 3: ROC Curves

## a. ROC curve: training data

Produce one plot that has the ROC curves, using the *training data*, for all three models (linear, logistic, and random forest). Use color and/or linetype to distinguish between models and include a legend.    
Also report the AUC (area under the ROC curve) for each model. Again, use the *training data*. 

- Note: you should be weary of being asked to evaluation predictive performance from the same data used to estimate the tuning and model parameters. The next problem will walk you through a more proper way of evaluating predictive performance with resampling. 

::: {.callout-note title="Solution"}




```{r prob3a}

linear_preds <- predict(cv_1, newx = x_train)

log_preds <- predict(cv_2, newx = x_train, s = "lambda.min", type = "response")

rf_preds <- predict(rf_model2, newx = x_train, type = "prob")[, 2]  

linear_roc <- roc(y_train, as.numeric(linear_preds), quiet = TRUE)
log_roc <- roc(y_train, as.numeric(log_preds), quiet = TRUE)
rf_roc <- roc(y_train, as.numeric(rf_preds), quiet = TRUE)

linear_auc <- auc(linear_roc)
logistic_auc <- auc(log_roc)
rf_auc <- auc(rf_roc)

cat("Linear model AUC: ", linear_auc, "\n")
cat("Log regression AUC: ", logistic_auc, "\n")
cat("Random forest AUC: ", rf_auc, "\n")
```

```{r debug}
cat("Rows in X_train:", nrow(x_train), "\n")
cat("Rows in linear_preds:", length(linear_preds), "\n")
cat("Rows in logistic_preds:", length(log_preds), "\n")
cat("Rows in rf_preds:", length(rf_preds), "\n")
```

```{r prob3a rocs}
  fpr_linear = linear_roc$specificities
  tpr_linear = linear_roc$sensitivities
  
  fpr_logistic = log_roc$specificities
  tpr_logistic = log_roc$sensitivities
  
  fpr_rf = rf_roc$specificities
  tpr_rf = rf_roc$sensitivities

```



```{r prob 3a plot}

ggplot() +
  geom_line(aes(x = 1 - linear_roc$specificities, y = linear_roc$sensitivities, color = "Linear Model")) +
  geom_line(aes(x = 1 - log_roc$specificities, y = log_roc$sensitivities, color = "Logistic Regression")) +
  geom_line(aes(x = 1 - rf_roc$specificities, y = rf_roc$sensitivities, color = "Random Forest")) +
  labs(title = "ROC Curves for 3 models",
       x = "FPR (1 - Specificity)",
       y = "TPR (Sensitivity)") +
  scale_color_manual(name = "Model", values = c("Linear Model" = "blue", "Logistic Regression" = "red", "Random Forest" = "green")) +
  theme_minimal()

```
:::


## b. ROC curve: resampling estimate

Recreate the ROC curve from the penalized logistic regression (logreg) and random forest (rf) models using repeated hold-out data. The following steps will guide you:

- For logreg, use $\alpha=.75$. For rf use *mtry = 2*,  *num.trees = 1000*, and fix any other tuning parameters at your choice. 
- Run the following steps 25 times:
    i. Hold out 500 observations.
    ii. Use the remaining observations to estimate $\lambda$ using 10-fold CV for the logreg model. Don't tune any rf parameters.
    iii. Predict the probability of linkage for the 500 hold-out observations.
    iv. Store the predictions and hold-out labels.
    v. Calculate the AUC. 
- Report the mean AUC and standard error for both models. Compare to the results from part a. 
- Produce two plots showing the 25 ROC curves for each model. 
- Note: by estimating $\lambda$ each iteration, we are incorporating the uncertainty present in estimating that tuning parameter. 
    
::: {.callout-note title="Solution"} 
```{r prob3b settings}

n_rounds <- 25
hold_out <- 500
alpha_logreg <- 0.75
mtry_rf <- 2
num_trees <- 1000

auc_logreg <- numeric(n_rounds)
auc_rf <- numeric(n_rounds)

roc_curves_logreg <- list()
roc_curves_rf <- list()
```

```{r prob3b iter loop}
set.seed(321) 
for (i in 1:n_rounds) {
  holdout_indices <- sample(seq_len(nrow(x_train)), size = hold_out)
  x_holdout <- x_train[holdout_indices, ]
  y_holdout <- y_train[holdout_indices]
  
  x_train2 <- x_train[-holdout_indices, ]
  y_train2 <- y_train[-holdout_indices]
  
  logreg_cv <- cv.glmnet(x_train, y_train, alpha = alpha_logreg, family = "binomial", nfolds = 10)
  logreg_preds <- predict(logreg_cv, newx = x_holdout, s = "lambda.min", type = "response")
  
  
  rf_model <- randomForest(x_train, as.factor(y_train), mtry = mtry_rf, ntree = num_trees)
  rf_preds <- predict(rf_model, x_holdout, type = "prob")[, 2]  #
 
  
  roc_logreg <- roc(y_holdout, as.numeric(logreg_preds), quiet = TRUE)
  roc_rf <- roc(y_holdout, rf_preds, quiet=TRUE)

  auc_logreg[i] <- auc(roc_logreg)
  auc_rf[i] <- auc(roc_rf)
  
  roc_curves_logreg[[i]] <- roc_logreg
  roc_curves_rf[[i]] <- roc_rf
}
```

```{r prob 3b cont2}
mean_auc_logreg <- mean(auc_logreg)
se_auc_logreg <- sd(auc_logreg) / sqrt(n_rounds)

mean_auc_rf <- mean(auc_rf)
se_auc_rf <- sd(auc_rf) / sqrt(n_rounds)

cat("Log Reg - Mean AUC:", mean_auc_logreg, "SE:", se_auc_logreg, "\n")
cat("RF - Mean AUC:", mean_auc_rf, "SE:", se_auc_rf, "\n")
```

```{r prob3b plot}
## this object plots logreg roc
plot_roc_logreg <- ggplot() +
  labs(title = "ROC Curves - Penalized Log Reg", x = "FPR", y = "TPR") +
  theme_minimal()
## this for loop plots logreg roc for ever iter
for (i in 1:n_rounds) {
  plot_roc_logreg <- plot_roc_logreg +
    geom_line(aes(x = 1 - roc_curves_logreg[[i]]$specificities, y = roc_curves_logreg[[i]]$sensitivities), alpha = 0.3, color = "blue")
}
## this object plots rf roc
plot_roc_rf <- ggplot() +
  labs(title = "ROC Curves - RF", x = "FPR", y = "TPR") +
  theme_minimal()
## this for loop plots rf roc for every iter
for (i in 1:n_rounds) {
  plot_roc_rf <- plot_roc_rf +
    geom_line(aes(x = 1 - roc_curves_rf[[i]]$specificities, y = roc_curves_rf[[i]]$sensitivities), alpha = 0.3, color = "green")
}

print(plot_roc_logreg)
print(plot_roc_rf)

```
:::

# Problem 4: Contest

## a. Contest Part 1: Predict the estimated *probability* of linkage. 

Predict the estimated *probability* of linkage for the test data (using any model). 

- Submit a .csv file (ensure comma separated format) named `lastname_firstname_1.csv` that includes the column named **p** that is your estimated posterior probability. We will use automated evaluation, so the format must be exact. 
- You are free to any model (even ones we haven't yet covered in the course).
- You are free to use any data transformation or feature engineering.
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points.     
- Your probabilities will be evaluated with respect to the mean negative Bernoulli log-likelihood (known as the average *log-loss* metric):
$$ 
L = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$
where $M$ is the number of test observations, $\hat{p}_i$ is the prediction for the $i$th test observation, and $y_i \in \{0,1\}$ are the true test set labels. 

::: {.callout-note title="Solution"}
```{r prob4a}
x_train <- x_train[, c("spatial", "temporal", "tod", "dow", "LOC", "POA", "MOA", "TIMERANGE")]
x_test <- as.matrix(linkage_test)

set.seed(312)
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.64, family = "binomial", nfolds = 25)

predicts <- predict(elastic_net_model, newx = x_test, s = "lambda.min", type = "response")

head(predicts)
```

:::


## b. Contest Part 2: Predict the *linkage label*. 

Predict the linkages for the test data (using any model). 

- Submit a .csv file (ensure comma separated format) named `lastname_firstname_2.csv` that includes the column named **linkage** that takes the value of 1 for linked pairs and 0 for unlinked pairs. We will use automated evaluation, so the format must be exact. 
- You are free to any model (even ones we haven't yet covered in the course).
- You are free to use any data transformation or feature engineering.
- Your labels will be evaluated based on total cost, where cost is equal to `1*FP + 8*FN`. This implies that False Negatives (FN) are 8 times as costly as False Positives (FP).    
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points. Note: you only will get bonus credit for one of the two contests. 

::: {.callout-note title="Solution"}
```{r prob4b}

set.seed(321) 
rf_model <- randomForest(x_train, as.factor(y_train), ntree = 1000, mtry = 2, importance = TRUE)
rf_train_probs <- predict(rf_model, x_train, type = "prob")[, 2]

roc_curve <- roc(y_train, rf_train_probs, quiet = TRUE)
thresholds <- coords(roc_curve, "all", ret = c("threshold", "specificity", "sensitivity"))

cost <- (1 - thresholds$specificity) + 8 * (1 - thresholds$sensitivity)
min_cost <- which.min(cost)
my_threshold <- thresholds$threshold[min_cost]

cat("Optimal Threshold:", my_threshold, "\n")

x_test <- as.matrix(linkage_test) 
rf_test_probs <- predict(rf_model, x_test, type = "prob")[, 2]

predicts2 <- ifelse(rf_test_probs >= my_threshold, 1, 0)
predicts2

```

```{r print csvs}
csv1 <- data.frame(p = predicts)
write.csv(csv1, file = "dane_kiana_2.csv", row.names = FALSE)

csv2 <- data.frame(p = predicts2)
write.csv(csv2, file = "dane_kiana_1.csv", row.names = FALSE)

```
:::

