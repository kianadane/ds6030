---
title: "Homework #7 - ATTEMPT 2" 
author: "Kiana Dane"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Stacking for Kaggle

You are to make at least one official entry in the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) Kaggle contest **using stacking or model averaging**; at least one component model must be a boosting model. 

- You will need to register in Kaggle (its free)
- Read the details of the contest. Understand the data and evaluation function.
- Make at least one submission that uses **stacking or model averaging**. 
- If you get a score on the public leaderboard of $\text{RMSE}<0.50$ (note RMSE is calculated on the log scale), you receive full credit, otherwise, you'll lose 10 points. 
    - I'll allow [teaming](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/team). Each team member can produce one component model and then use stacking or model averaging to combine predictions. 
    - You don't need to team, but must still combine multiple models. At least one of the component models should be boosting. 
- Each person submit the following in Canvas:
    - Code (if teaming, your code and the shared stacking code)
    - kaggle name (or team name) so we can ensure you had a valid submission. 
    - your score and current ranking on the kaggle leaderboard
- Top 5 scores get 2 bonus points
    - Teams will split their bonus points among team members
```{r}
library(tidyverse)
library(glmnet)
library(xgboost)
library(caret)

train <- read.csv("train.csv")
test <- read.csv("test.csv")


# LOG TRANSFORM - REQUIRED 
train$SalePrice <- log(train$SalePrice)

train_x <- train %>% select(-SalePrice, -Id)
train_y <- train$SalePrice

# CV HERE
set.seed(123)
train_control <- trainControl(method = "cv", number = 7)

# MODEL 1 - LASSO HERE 
lasso_model <- train(train_x, train_y, method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 0.1, length = 10)), trControl = train_control)

# MODEL 2 - LINEAR HERE
lm_model <- train(train_x, train_y, method = "lm", trControl = train_control)

# MODEL 3 = BOOSTING
xgb_grid <- expand.grid(nrounds = 100, eta = 0.1, max_depth = 6, gamma = 0, colsample_bytree = 0.7, min_child_weight = 1, subsample = 0.7)

xgb_model <- train(train_x, train_y, method = "xgbTree", tuneGrid = xgb_grid, trControl = train_control)

```

```{r}
# Predicting on the test set with the best model
best_model <- xgb_model  # replace with lm_model or lasso_model if those perform better
test_x <- test %>% select(-Id)
test_preds <- predict(best_model, newdata = test_x)

# Convert predictions back from log scale
final_preds <- exp(test_preds)

# Preparing submission file
submission <- data.frame(Id = test$Id, SalePrice = final_preds)
write.csv(submission, "submission.csv", row.names = FALSE)


```

```{r stacking - meta model}

# Cross-validated predictions on training data
train_preds_lm <- predict(lm_model, newdata = train_x)
train_preds_lasso <- predict(lasso_model, newdata = train_x)
train_preds_xgb <- predict(xgb_model, newdata = train_x)

# Creating a new training dataset for the meta-model
meta_train <- data.frame(
  lm = train_preds_lm,
  lasso = train_preds_lasso,
  xgb = train_preds_xgb
)

# The meta-model target is still SalePrice
meta_y <- train_y

# Train a meta-model (e.g., linear regression or xgboost) on base model predictions
meta_model <- train(
  meta_train, meta_y,
  method = "lm",  # Could also try "xgbTree" or other models
  trControl = train_control
)

# Generating test predictions for stacking
test_preds_meta <- data.frame(
  lm = lm_preds,
  lasso = lasso_preds,
  xgb = xgb_preds
)

# Predict with the meta-model
stacked_preds <- predict(meta_model, newdata = test_preds_meta)

# Convert predictions back from log scale
final_preds <- exp(stacked_preds)

```

Note: Check out the [Kaggle notebooks](https://www.kaggle.com/docs/notebooks) which let you make submissions directly from the notebook. Its very similar to using Rivanna's OnDemand in that you can make a RMarkdown/Jupyter notebook or R/Python scrips that run on the cloud. Free CPU (4 cores, 30GB RAM) - amazing! Let your laptops cool off after all their hard work this semester.  
